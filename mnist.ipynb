{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "        \n",
    "    return test_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.297074\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.293869\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.133208\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.238080\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.143395\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.214506\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.121252\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.143907\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.151132\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.148911\n",
      "\n",
      "Test set: Average loss: 0.0535, Accuracy: 9824/10000 (98%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.023108\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.168773\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.116497\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.157444\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.087692\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.041113\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.089058\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.106648\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.193459\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.023748\n",
      "\n",
      "Test set: Average loss: 0.0376, Accuracy: 9875/10000 (99%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.022937\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.157654\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.009903\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.069547\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.016282\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.034873\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.067325\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.008056\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.140778\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.108204\n",
      "\n",
      "Test set: Average loss: 0.0357, Accuracy: 9875/10000 (99%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.013571\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.131788\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.040347\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.091592\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.001758\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.013153\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.036292\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.010249\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.112829\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.083032\n",
      "\n",
      "Test set: Average loss: 0.0304, Accuracy: 9895/10000 (99%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.007011\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.221678\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.037585\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.058176\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.013896\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.026655\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.023362\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.019744\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.077740\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.056104\n",
      "\n",
      "Test set: Average loss: 0.0322, Accuracy: 9892/10000 (99%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.116786\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.078393\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.022014\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.049655\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.016221\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.035871\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.015147\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.013880\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.116083\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.016625\n",
      "\n",
      "Test set: Average loss: 0.0278, Accuracy: 9906/10000 (99%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.014536\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.204122\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.018587\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.081327\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.011128\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.033794\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.016714\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.016353\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.109764\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.049766\n",
      "\n",
      "Test set: Average loss: 0.0287, Accuracy: 9909/10000 (99%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.009016\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.190169\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.031337\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.145006\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.004722\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.089433\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.033415\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.008900\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.078026\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.016489\n",
      "\n",
      "Test set: Average loss: 0.0267, Accuracy: 9913/10000 (99%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.025720\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.100852\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.029312\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.034465\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.067904\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.127792\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.070885\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.022155\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.151416\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.007587\n",
      "\n",
      "Test set: Average loss: 0.0261, Accuracy: 9916/10000 (99%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.026687\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.097911\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.014079\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.161015\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.022551\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.019826\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.041567\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.038423\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.022563\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.022321\n",
      "\n",
      "Test set: Average loss: 0.0254, Accuracy: 9915/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "device = torch.device(\"cuda\")\n",
    "train_kwargs = {'batch_size': 64}\n",
    "test_kwargs = {'batch_size': 64}\n",
    "\n",
    "transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "dataset1 = datasets.MNIST('data', train=True, download=True,\n",
    "                       transform=transform)\n",
    "dataset2 = datasets.MNIST('data', train=False,\n",
    "                       transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
    "\n",
    "model = Net().to(device)\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=1.0)\n",
    "\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.7)\n",
    "losses = []\n",
    "for epoch in range(1, epochs + 1):\n",
    "        train(model, device, train_loader, optimizer, epoch)\n",
    "        loss = test(model, device, test_loader)\n",
    "        scheduler.step()\n",
    "\n",
    "# save model\n",
    "torch.save(model.state_dict(), \"mnist_cnn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 28, 28])\n",
      "tensor([[0.0529, 0.0159, 0.0278, 0.0115, 0.1226, 0.1020, 0.0605, 0.0064, 0.5489,\n",
      "         0.0514]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([8], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "one = [[[\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "]]]\n",
    "\n",
    "plt.imshow(one[0][0])\n",
    "one = torch.Tensor(one).to(device)\n",
    "print(one.shape)\n",
    "result_tensor = model(one)\n",
    "probabilities = F.softmax(result_tensor, dim=1)\n",
    "predicted_class = torch.argmax(probabilities, dim=1)\n",
    "print(probabilities)\n",
    "print(predicted_class)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
